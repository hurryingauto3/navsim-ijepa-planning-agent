# hydra:
#   run:
#     dir: ${output_dir}
#   output_subdir: ${output_dir}/code/hydra           # Store hydra's config breakdown here for debugging
#   searchpath:                                       # Only <exp_dir> in these paths are discoverable
#     - pkg://navsim.planning.script.config.common
#   job:
#     chdir: False

# defaults:
#   - default_common
#   - default_evaluation
#   - default_train_val_test_log_split
#   - agent: ego_status_mlp_agent
#   - _self_

# split: trainval
# cache_path: ${oc.env:NAVSIM_EXP_ROOT}/training_cache
# use_cache_without_dataset: false # load the training samples from the cache. scene-filter will be ignored
# force_cache_computation: true
# seed: 0

# dataloader:
#   params:
#     batch_size: 64  # number of samples per batch
#     num_workers: 4  # number of workers for data loading
#     pin_memory: true  # pin memory for faster GPU transfer
#     prefetch_factor: 2  # number of samples loaded in advance by each worker

# trainer:
#   params:
#     max_epochs: 100  # maximum number of training epochs
#     check_val_every_n_epoch: 1  # run validation set every n training epochs
#     val_check_interval: 1.0  # [%] run validation set every X% of training set

#     limit_train_batches: 1.0  # how much of training dataset to check (float = fraction, int = num_batches)
#     limit_val_batches: 1.0  # how much of validation dataset to check (float = fraction, int = num_batches)

#     accelerator: gpu  # distribution method
#     strategy: ddp
#     precision: 16-mixed  # floating point precision
#     num_nodes: 1  # Number of nodes used for training

#     num_sanity_val_steps: 0  # number of validation steps to run before training begins
#     fast_dev_run: false  # runs 1 batch of train/val/test for sanity

#     accumulate_grad_batches: 1  # accumulates gradients every n batches
#     # track_grad_norm: -1  # logs the p-norm for inspection
#     gradient_clip_val: 0.0  # value to clip gradients
#     gradient_clip_algorithm: norm  # [value, norm] method to clip gradients
#     default_root_dir: ${output_dir}


hydra:
  run:
    dir: ${output_dir}
  output_subdir: ${output_dir}/code/hydra
  searchpath:
    - pkg://navsim.planning.script.config.common
  job:
    chdir: False

defaults:
  - default_common
  - default_evaluation
  - default_train_val_test_log_split
  - agent: ijepa_planning_agent # This now points to your new config file
  - _self_

# ... rest of the config remains largely the same ...

split: trainval
cache_path: ${oc.env:NAVSIM_EXP_ROOT}/training_cache
use_cache_without_dataset: false
force_cache_computation: true # Keep true if you want to regenerate cache with your new builders
seed: 0

dataloader:
  params:
    batch_size: 64
    num_workers: 8
    pin_memory: true
    prefetch_factor: 1
    # persistent_workers: true # Often good with num_workers > 0
    # collate_fn: navsim.common.dataloader.collate_fn_skip_none # Add this if your builders could return None (though the ones provided return zero tensors)

trainer:
  params:
    max_epochs: 30
    check_val_every_n_epoch: 5
    val_check_interval : 1.0

    limit_train_batches: 1.0
    limit_val_batches: 1.0

    accelerator: gpu
    strategy: auto
    precision: 16-mixed
    num_nodes: 1

    num_sanity_val_steps: 0
    fast_dev_run: false

    accumulate_grad_batches: 1
    gradient_clip_val: 0.0
    gradient_clip_algorithm: norm
    default_root_dir: ${output_dir}