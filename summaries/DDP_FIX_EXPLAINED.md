# üöÄ FIXED: Multi-GPU DDP Training

## ‚ùå What Was Wrong

All previous 8-GPU attempts failed because of **SLURM configuration mistakes**:

```bash
# WRONG APPROACH (what we tried):
#SBATCH --gres=gpu:h200:1        # ‚ùå Only requests 1 GPU!
#SBATCH --ntasks-per-node=8      # ‚ùå Launches 8 separate processes
```

**Problem**: PyTorch Lightning DDP expects:
1. Multiple GPUs visible to a **SINGLE** process
2. Lightning internally spawns worker processes

But we were giving SLURM 1 GPU and asking it to launch 8 tasks ‚Üí DDP hung waiting for GPUs that didn't exist.

---

## ‚úÖ Correct Solution

```bash
# CORRECT APPROACH:
#SBATCH --gres=gpu:8             # ‚úì Request 8 GPUs
#SBATCH --ntasks=1               # ‚úì Launch only 1 process
#SBATCH --cpus-per-task=128      # ‚úì All CPUs for this process

export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  # ‚úì Make all GPUs visible

# NOTE: run_training_dense.py HARDCODES strategy=DDPStrategy
# Lightning auto-detects 8 GPUs from CUDA_VISIBLE_DEVICES!

python run_training_dense.py \
    agent=hydra_mdp_v8192_w_ep \
    trainer.params.accelerator=gpu \    # ‚úì Use GPU
    trainer.params.num_nodes=1 \        # ‚úì Single node
    dataloader.params.batch_size=22     # ‚úì 22 per GPU = 176 total
```

---

## üéØ How Lightning DDP Works

1. **Main process** starts with all GPUs visible
2. Lightning **automatically spawns** 8 worker processes (one per GPU)
3. Each worker:
   - Gets assigned to 1 GPU (rank 0-7)
   - Loads data with its own DataLoader workers
   - Trains its own model replica
   - Syncs gradients with other ranks

**KEY**: You launch 1 Python process, Lightning handles the rest!

---

## üìä Expected Performance

| Setup | Batch Size | Speed | Time for 40 Epochs |
|-------|------------|-------|-------------------|
| **1 GPU (current)** | 22 | 0.78 it/s | ~80 hours |
| **8 GPUs (DDP)** | 176 (22√ó8) | ~6 it/s | **~10 hours** ‚ö° |

**8√ó speedup** because:
- 8√ó more samples per batch
- Near-linear scaling with DDP
- Better GPU utilization

---

## üöÄ Usage

### Submit 8-GPU Training
```bash
sbatch --account=torch_pr_68_tandon_advanced scripts/train_8gpu_ddp_fixed.slurm
```

### Monitor Progress
```bash
# Check job status
squeue -u $USER

# Watch training logs (once running)
tail -f /scratch/ah7072/navsim_workspace/exp/logs/train_<JOBID>.out
```

---

## üîç Key Differences from Failed Attempts

| Setting | ‚ùå Old (Failed) | ‚úÖ New (Fixed) |
|---------|----------------|----------------|
| `--gres` | `gpu:h200:1` | `gpu:8` |
| `--ntasks` | `8` | `1` |
| `--ntasks-per-node` | `8` | (removed) |
| `--cpus-per-task` | `16` | `128` |
| `trainer.params.devices` | ‚ùå Not in config! | (auto-detected) |
| `trainer.params.strategy` | ‚ùå Hardcoded in code! | (can't override) |
| `CUDA_VISIBLE_DEVICES` | (not set) | `0,1,2,3,4,5,6,7` |

**CRITICAL**: 
- `run_training_dense.py` **hardcodes** `strategy=DDPStrategy` (line 159)
- Config has NO `devices` parameter - Lightning **auto-detects** from environment!
- Setting `CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7` tells Lightning "use these 8 GPUs"

---

## üìù Troubleshooting

### If DDP still hangs:
```bash
# Check GPUs are visible
srun --jobid=<JOBID> --pty bash
python -c "import torch; print(f'GPUs: {torch.cuda.device_count()}')"
# Should print: GPUs: 8
```

### If OOM errors:
```bash
# Reduce batch size per GPU
python run_training_dense.py \
    dataloader.params.batch_size=16  # Reduce from 22 to 16
```

### If "Address already in use":
```bash
# Lightning auto-finds free port, but if it fails:
python run_training_dense.py \
    +trainer.params.strategy.find_unused_parameters=false
```

---

## üéì Learning: SLURM vs Lightning

**SLURM's job**: Allocate resources (GPUs, CPUs, memory)
**Lightning's job**: Distribute training across those resources

Don't mix SLURM's `--ntasks` with Lightning's `devices` - let Lightning handle process spawning!

---

## Next Steps

1. ‚úÖ Submit `train_8gpu_ddp_fixed.slurm`
2. ‚è≥ Wait ~10 hours for training
3. üéâ Complete 40 epochs 8√ó faster!
4. üìä Evaluate with PDM score
