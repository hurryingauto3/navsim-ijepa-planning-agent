name: Deploy Web Showcase

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    env:
      TF_WORKDIR: web/infra/aws
      AWS_REGION: ${{ secrets.AWS_REGION != '' && secrets.AWS_REGION || 'us-east-1' }}
      AWS_INSTANCE_TYPE: ${{ secrets.AWS_INSTANCE_TYPE != '' && secrets.AWS_INSTANCE_TYPE || 't3.micro' }}
      TF_VAR_ssh_key_name: ${{ secrets.AWS_SSH_KEY_NAME }}
      TF_VAR_allow_ssh_cidr: ${{ secrets.AWS_SSH_ALLOWED_CIDR != '' && secrets.AWS_SSH_ALLOWED_CIDR || '0.0.0.0/0' }}
      TF_VAR_route53_zone_id: ${{ secrets.ROUTE53_ZONE_ID != '' && secrets.ROUTE53_ZONE_ID || '' }}
      TF_VAR_subdomain: ${{ secrets.ROUTE53_SUBDOMAIN != '' && secrets.ROUTE53_SUBDOMAIN || '' }}
      TF_VAR_project_name: navsim-web
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Cleanup orphaned resources
        run: |
          echo "Cleaning up any orphaned resources from previous failed runs..."
          
          # Step 1: Terminate any existing EC2 instances with navsim-web name
          echo "Checking for existing EC2 instances..."
          INSTANCE_IDS=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=navsim-web" "Name=instance-state-name,Values=running,pending,stopping,stopped" \
            --query 'Reservations[*].Instances[*].InstanceId' \
            --output text 2>/dev/null | xargs || echo "")
          
          if [ -n "$INSTANCE_IDS" ]; then
            echo "Found existing EC2 instances: $INSTANCE_IDS"
            echo "Terminating instances..."
            for INSTANCE_ID in $INSTANCE_IDS; do
              aws ec2 terminate-instances --instance-ids "$INSTANCE_ID" 2>&1 || true
              echo "Terminated instance: $INSTANCE_ID"
            done
            echo "Waiting 30 seconds for instances to terminate..."
            sleep 30
          else
            echo "No existing EC2 instances found"
          fi
          
          # Step 2: Clean up security group - try multiple methods to find and delete it
          echo "Checking for existing security groups named 'navsim-web-sg'..."
          
          # Method 1: Query by name
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=navsim-web-sg" \
            --query 'SecurityGroups[*].GroupId' \
            --output text 2>/dev/null | head -n1 | xargs || echo "")
          
          # Method 2: If that didn't work, try to find by tag
          if [ -z "$SG_ID" ] || [ "$SG_ID" = "None" ]; then
            SG_ID=$(aws ec2 describe-security-groups \
              --filters "Name=tag:Name,Values=navsim-web-sg" \
              --query 'SecurityGroups[*].GroupId' \
              --output text 2>/dev/null | head -n1 | xargs || echo "")
          fi
          
          if [ -n "$SG_ID" ] && [ "$SG_ID" != "None" ] && [[ "$SG_ID" == sg-* ]]; then
            echo "Found security group: $SG_ID"
            echo "Deleting security group: $SG_ID"
            if aws ec2 delete-security-group --group-id "$SG_ID" 2>&1; then
              echo "✓ Security group deleted successfully"
            else
              echo "⚠ Security group deletion failed (will try Terraform import/destroy)"
            fi
          else
            echo "No security group found to delete"
          fi
          
          # Clean up IAM instance profile (must be done before role)
          INSTANCE_PROFILE=$(aws iam get-instance-profile \
            --instance-profile-name navsim-web-instance-profile \
            --query 'InstanceProfile.InstanceProfileName' \
            --output text 2>/dev/null || echo "")
          
          if [ "$INSTANCE_PROFILE" != "None" ] && [ -n "$INSTANCE_PROFILE" ]; then
            echo "Removing role from instance profile: $INSTANCE_PROFILE"
            aws iam remove-role-from-instance-profile \
              --instance-profile-name "$INSTANCE_PROFILE" \
              --role-name navsim-web-ec2-role 2>&1 || echo "Role removal failed or already removed"
            
            echo "Deleting instance profile: $INSTANCE_PROFILE"
            aws iam delete-instance-profile \
              --instance-profile-name "$INSTANCE_PROFILE" 2>&1 || echo "Instance profile deletion failed or already deleted"
          else
            echo "No instance profile found to delete"
          fi
          
          # Clean up IAM role (detach policies first)
          ROLE_EXISTS=$(aws iam get-role \
            --role-name navsim-web-ec2-role \
            --query 'Role.RoleName' \
            --output text 2>/dev/null || echo "")
          
          if [ "$ROLE_EXISTS" != "None" ] && [ -n "$ROLE_EXISTS" ]; then
            echo "Detaching policies from role: navsim-web-ec2-role"
            aws iam detach-role-policy \
              --role-name navsim-web-ec2-role \
              --policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore 2>&1 || echo "Policy already detached or doesn't exist"
            
            echo "Deleting IAM role: navsim-web-ec2-role"
            aws iam delete-role \
              --role-name navsim-web-ec2-role 2>&1 || echo "Role deletion failed or already deleted"
          else
            echo "No IAM role found to delete"
          fi
          
          echo "✓ Cleanup complete"

      - name: Terraform Init
        working-directory: ${{ env.TF_WORKDIR }}
        run: terraform init

      - name: Terraform Destroy orphaned resources (if any)
        working-directory: ${{ env.TF_WORKDIR }}
        continue-on-error: true
        run: |
          # Try to import existing resources into state, then destroy them
          # This handles the case where resources exist but aren't in Terraform state
          
          # Check if security group exists and try to import it
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=navsim-web-sg" \
            --query 'SecurityGroups[*].GroupId' \
            --output text 2>/dev/null | head -n1 | xargs || echo "")
          
          if [ -n "$SG_ID" ] && [[ "$SG_ID" == sg-* ]]; then
            echo "Attempting to import existing security group: $SG_ID"
            terraform import aws_security_group.navsim "$SG_ID" 2>&1 || echo "Import failed (may not be in state)"
            terraform destroy -target=aws_security_group.navsim -auto-approve 2>&1 || echo "Destroy failed (may not be in state)"
          fi
          
          # Check if IAM role exists and try to import it
          ROLE_EXISTS=$(aws iam get-role \
            --role-name navsim-web-ec2-role \
            --query 'Role.RoleName' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$ROLE_EXISTS" ] && [ "$ROLE_EXISTS" != "None" ]; then
            echo "Attempting to import existing IAM role: navsim-web-ec2-role"
            terraform import aws_iam_role.navsim navsim-web-ec2-role 2>&1 || echo "Import failed (may not be in state)"
            terraform destroy -target=aws_iam_role.navsim -auto-approve 2>&1 || echo "Destroy failed (may not be in state)"
          fi

      - name: Terraform Apply
        working-directory: ${{ env.TF_WORKDIR }}
        run: |
          terraform apply -auto-approve \
            -var "aws_region=${{ env.AWS_REGION }}" \
            -var "instance_type=${{ env.AWS_INSTANCE_TYPE }}" \
            -var "ssh_key_name=${{ secrets.AWS_SSH_KEY_NAME }}" \
            -var "allow_ssh_cidr=${{ secrets.AWS_SSH_ALLOWED_CIDR || '0.0.0.0/0' }}" \
            -var "route53_zone_id=${{ secrets.ROUTE53_ZONE_ID || '' }}" \
            -var "subdomain=${{ secrets.ROUTE53_SUBDOMAIN || '' }}"

      - name: Capture Terraform Outputs
        id: tf
        working-directory: ${{ env.TF_WORKDIR }}
        run: |
          echo "public_ip=$(terraform output -raw public_ip)" >> "$GITHUB_OUTPUT"
          echo "instance_id=$(terraform output -raw instance_id)" >> "$GITHUB_OUTPUT"

      - name: Start SSH Agent
        uses: webfactory/ssh-agent@v0.8.0
        with:
          ssh-private-key: ${{ secrets.AWS_SSH_KEY }}

      - name: Wait for EC2 SSH to be ready
        run: |
          echo "Waiting for EC2 instance SSH to be ready..."
          PUBLIC_IP="${{ steps.tf.outputs.public_ip }}"
          
          # Wait up to 5 minutes for SSH to be available
          for i in {1..30}; do
            if ssh-keyscan -H "$PUBLIC_IP" >> ~/.ssh/known_hosts 2>&1 && \
               ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o BatchMode=yes ec2-user@"$PUBLIC_IP" exit 0 2>&1; then
              echo "✓ SSH is ready after $((i*10)) seconds"
              exit 0
            fi
            echo "Attempt $i/30: SSH not ready yet, waiting 10 seconds..."
            sleep 10
          done
          
          echo "❌ SSH connection failed after 5 minutes"
          exit 1

      - name: Upload web bundle
        run: |
          PUBLIC_IP="${{ steps.tf.outputs.public_ip }}"
          echo "Packaging web workspace and streaming to EC2..."
          ssh ec2-user@"$PUBLIC_IP" "rm -rf ~/navsim-web && mkdir -p ~/navsim-web"
          tar -C web \
            --exclude 'frontend/node_modules' \
            --exclude 'frontend/.next' \
            --exclude '.git' \
            --exclude '.terraform' \
            -czf - . \
            | ssh ec2-user@"$PUBLIC_IP" "tar -C ~/navsim-web -xzf -"

      - name: Build and restart compose stack
        run: |
          ssh ec2-user@${{ steps.tf.outputs.public_ip }} <<'EOF'
          set -euxo pipefail
          cd /home/ec2-user/navsim-web/infra/aws
          sudo docker compose -f docker-compose.aws.yml build --pull
          sudo docker compose -f docker-compose.aws.yml up -d --remove-orphans
          EOF

      - name: Cleanup on failure
        if: failure()
        run: |
          echo "Workflow failed - cleaning up any resources that were created..."
          
          # Try to get instance ID from Terraform state if it exists
          if [ -f "${{ env.TF_WORKDIR }}/terraform.tfstate" ]; then
            INSTANCE_ID=$(cd ${{ env.TF_WORKDIR }} && terraform output -raw instance_id 2>/dev/null || echo "")
            if [ -n "$INSTANCE_ID" ] && [ "$INSTANCE_ID" != "" ]; then
              echo "Terminating EC2 instance: $INSTANCE_ID"
              aws ec2 terminate-instances --instance-ids "$INSTANCE_ID" 2>&1 || echo "Instance termination failed or already terminated"
            fi
          fi
          
          # Run the same cleanup as pre-deploy
          echo "Cleaning up security group, IAM role, and instance profile..."
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=navsim-web-sg" \
            --query 'SecurityGroups[0].GroupId' \
            --output text 2>/dev/null || echo "")
          
          if [ "$SG_ID" != "None" ] && [ -n "$SG_ID" ]; then
            aws ec2 delete-security-group --group-id "$SG_ID" 2>&1 || true
          fi
          
          INSTANCE_PROFILE=$(aws iam get-instance-profile \
            --instance-profile-name navsim-web-instance-profile \
            --query 'InstanceProfile.InstanceProfileName' \
            --output text 2>/dev/null || echo "")
          
          if [ "$INSTANCE_PROFILE" != "None" ] && [ -n "$INSTANCE_PROFILE" ]; then
            aws iam remove-role-from-instance-profile \
              --instance-profile-name "$INSTANCE_PROFILE" \
              --role-name navsim-web-ec2-role 2>&1 || true
            aws iam delete-instance-profile \
              --instance-profile-name "$INSTANCE_PROFILE" 2>&1 || true
          fi
          
          ROLE_EXISTS=$(aws iam get-role \
            --role-name navsim-web-ec2-role \
            --query 'Role.RoleName' \
            --output text 2>/dev/null || echo "")
          
          if [ "$ROLE_EXISTS" != "None" ] && [ -n "$ROLE_EXISTS" ]; then
            aws iam detach-role-policy \
              --role-name navsim-web-ec2-role \
              --policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore 2>&1 || true
            aws iam delete-role \
              --role-name navsim-web-ec2-role 2>&1 || true
          fi
          
          echo "✓ Cleanup on failure complete"

