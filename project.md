I'll compile a literature review focusing on I-JEPA and its applications, particularly in self-supervised learning for autonomous driving. This will include:

- A deep dive into I-JEPA, its core principles, and how it compares to other self-supervised learning methods (e.g., MAE, SimCLR, BYOL).
- Analyzing prior work that integrates I-JEPA with autonomous driving tasks, even if not directly related to CARLA.
- Strategies for label-efficient fine-tuning and adaptation to the CARLA simulator.
- Key benchmarking methodologies, dataset requirements, and evaluation metrics relevant to CARLA-based self-driving tasks.

I'll gather and structure this information to ensure you have a comprehensive foundation for your literature review. I'll update you once the research is complete.

# I-JEPA Overview

## Core Principles of I-JEPA  
The **Implicit (Image-based) Joint-Embedding Predictive Architecture (I-JEPA)** is a self-supervised learning framework that predicts *high-level representations* of missing parts of an input from the remaining context. In I-JEPA, a **context encoder** processes a visible portion (context block) of an image, and a separate **target encoder** (often a momentum/EMA copy of the context encoder) processes other withheld parts (target blocks). A lightweight predictor network then learns to **predict the target encodings from the context encoding** ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=generative%20approach%20for%20self,from%20linear%20classification%20to%20object)) ([Meta AI’s I-JEPA Explained  | Encord](https://encord.com/blog/i-jepa-explained/#:~:text=Context%20Block)). Crucially, this prediction is done in the **latent feature space** rather than pixel space – the loss (e.g. an L2 loss) directly compares the predicted embedding to the target block’s embedding ([Meta AI’s I-JEPA Explained  | Encord](https://encord.com/blog/i-jepa-explained/#:~:text=Prediction)). By predicting *representations* of unseen regions, I-JEPA forces the model to capture the **semantic content** needed to infer those regions, without having to generate low-level pixel details ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Compared%20to%20generative%20methods%20that,JEPA%20towards%20producing%20semantic)). This leads to learning of abstract, semantic features. Another key to I-JEPA’s success is its **masking strategy**: the target regions must be large enough and the context sufficiently informative (e.g. multiple disjoint context patches) to require semantic understanding ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=various%20target%20blocks%20in%20the,object%20counting%20and%20depth%20prediction)) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=potentially%20eliminated%2C%20thereby%20leading%20the,spatially%20distributed%29%20context%20block)). In practice, I-JEPA has been implemented with Vision Transformers (ViT) for both context/target encoders, and has been shown to scale well – for example, training a ViT-H/14 on ImageNet in <72 hours on 16 GPUs achieved strong downstream results across classification, object counting, and depth prediction tasks ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=we%20find%20I,object%20counting%20and%20depth%20prediction)) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,1K%2C%20and%20semantic%20transfer%20tasks)).

**I-JEPA vs. Generative and Contrastive Approaches:** Unlike generative self-supervised methods (e.g. Masked Autoencoders) that learn by **reconstructing raw pixels** of masked patches ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=core%20of%20self,in%20transfer%20settings%20with%20limited)), I-JEPA *predicts higher-level representations* of masked regions ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Compared%20to%20generative%20methods%20that,JEPA%20towards%20producing%20semantic)). This means I-JEPA does not waste capacity on low-level details (like textures or exact pixel colors) and instead focuses on meaningful structure ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Compared%20to%20generative%20methods%20that,JEPA%20towards%20producing%20semantic)). It thus produces more semantic features than pixel-level reconstruction methods ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Compared%20to%20generative%20methods%20that,JEPA%20towards%20producing%20semantic)). Similarly, unlike **contrastive/invariance-based methods** (SimCLR, BYOL, etc.) that learn by making embeddings of two augmented views of an image similar ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Invariance,For)), I-JEPA does *not rely on hand-crafted data augmentations or view pairs*. Traditional contrastive methods require aggressive augmentations (cropping, color jitter, etc.) to create different views ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Invariance,For)) and often need explicit negative samples or special architectures to prevent collapse ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=The%20main%20challenge%20with%20JEAs,24%20%2C%20%204%2C%208)). I-JEPA avoids these by using a single context and target from the same image, eliminating the need for defining positive/negative pairs ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=masked%20unknown%20regions%2C%20our%20self,We%20qualitatively)). In essence, I-JEPA is **non-contrastive and non-generative** ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=Predictive%20Architecture%29%2C%20a%20novel%20self,We%20qualitatively)): it doesn’t require contrasting against negatives or reconstructing pixels. This simplicity leads to easier scalability and applicability across domains. In summary, I-JEPA’s core principle is learning by *predicting missing content in latent space* using a joint embedding framework, which sets it apart from prior self-supervised paradigms ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Image%3A%20Refer%20to%20caption%20,Embedding%20Architecture)) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Architectures%20learn%20to%20output%20similar,variables%20to%20facilitate%20prediction)).

## Differences from MAE, SimCLR, and BYOL  
I-JEPA’s design contrasts with popular self-supervised architectures in several ways:

- **No Pixel Reconstruction:** MAE (Masked Autoencoder) learns by reconstructing masked image patches in the pixel domain. It often captures low-level textures, yielding less semantic representations ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=either%20at%20the%20pixel%20or,full%20advantage%20of%20these%20methods)). I-JEPA instead predicts embeddings, which filters out pixel-level noise and encourages semantic feature learning ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Compared%20to%20generative%20methods%20that,JEPA%20towards%20producing%20semantic)). As a result, I-JEPA’s features tend to be of higher semantic level than MAE’s, which is evidenced by I-JEPA outperforming MAE on semantic benchmarks like ImageNet classification ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,1K%2C%20and%20semantic%20transfer%20tasks)).

- **No Explicit Contrastive Pairs:** SimCLR uses pairs of heavily augmented views and a contrastive loss with negatives to train an encoder, and BYOL similarly uses two augmented views (with a momentum encoder) to encourage view-invariant embeddings. These methods aim for *invariance* to image augmentations, which can discard potentially useful information and impose biases tied to chosen augmentations ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=image%20views%20typically%20constructed%20using,is%20not%20straightforward%20to%20generalize)). I-JEPA does not require multiple augmented views at all – it uses one view (one image, split into context/target) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=semantic%20image%20representations,Small%2F16%20trained%20with%20iBOT)). There is no need to generate negative samples or worry about collapse via view invariances; the prediction task itself provides the learning signal. In effect, I-JEPA sidesteps the complex dynamics of contrastive learning (no large batch negatives or momentum contrast needed) ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=masked%20unknown%20regions%2C%20our%20self,We%20qualitatively)), yet still avoids collapse by predicting a *non-trivial target* (the withheld content’s embedding) and by using a target network (EMA) similarly to BYOL for stability ([Meta AI’s I-JEPA Explained  | Encord](https://encord.com/blog/i-jepa-explained/#:~:text=The%20target%20block%20represents%20the,encoder%2C%20rather%20than%20the%20input)) ([Meta AI’s I-JEPA Explained  | Encord](https://encord.com/blog/i-jepa-explained/#:~:text=gradient,encoder%20parameters)).

- **Use of Context:** In SimCLR/BYOL, two views are usually global crops of the image, each containing most of the scene content just altered. I-JEPA instead uses a *partial observation* (context block) to predict the rest, which is more aligned with how a human might infer a missing part of a scene. This forces the model to truly understand scene structure, rather than just learn view-invariant object identity. It also means I-JEPA can capture details that contrastive methods might ignore due to enforcing invariances. For example, SimCLR/BYOL might treat two different parts of an image as the same “instance” (since they come from one image), possibly glossing over spatial relationships, whereas I-JEPA explicitly models those relationships by predicting specific missing regions.

- **Architectural Simplicity and Efficiency:** By using only one image view, I-JEPA is computationally efficient. Methods like SimCLR, BYOL, DINO, etc., effectively process *two* augmented images per example (or more) and sometimes additional projection heads, which increases compute. I-JEPA processes one context (and computes a few target patch embeddings) per image ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=style%20patch,Small%2F16%20trained%20with%20iBOT)). A study showed that **I-JEPA with a large ViT model required *less compute* than even a much smaller model trained with iBOT (a contrastive + masked hybrid method) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=semantic%20image%20representations,Small%2F16%20trained%20with%20iBOT))**. In fact, pre-training a ViT-H/14 with I-JEPA took under 1200 GPU-hours, which is faster than what prior methods needed for smaller architectures ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,supervised%20pretraining)). This efficiency comes from predicting in latent space (no heavy decoder as in MAE) and using a single view (less redundancy than multi-view methods) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,supervised%20pretraining)).

Overall, I-JEPA differentiates itself by *predicting abstract representations* instead of matching two views or reconstructing pixels. This leads to more semantic features than MAE ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Compared%20to%20generative%20methods%20that,JEPA%20towards%20producing%20semantic)) and avoids the augmentation biases of SimCLR/BYOL ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Invariance,For)), while being relatively simple and scalable ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,supervised%20pretraining)). The next section provides a more detailed comparison along specific dimensions (features, generalization, efficiency, performance).

# Comparisons with Other Self-Supervised Learning Models (MAE, SimCLR, BYOL)

To understand I-JEPA’s strengths, we compare it with **Masked Autoencoders (MAE)** as a representative generative method, and **SimCLR** and **BYOL** as representative contrastive/invariant methods, across several aspects:

- **Feature Representations:** I-JEPA learns features that are highly **semantic and contextual**. Because it predicts missing content in representation space, it discards low-level details and focuses on object-level or scene-level information ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Compared%20to%20generative%20methods%20that,JEPA%20towards%20producing%20semantic)). SimCLR/BYOL features are also semantic (especially BYOL/DINO, which can capture object identity), but they enforce invariance to viewpoint and appearance changes, which sometimes means losing fine-grained spatial information ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=amongst%20others%C2%A0,is%20not%20straightforward%20to%20generalize)). MAE features, on the other hand, tend to be more low-level – since MAE trains to reconstruct pixels, the encoder must preserve texture/structure details to solve that task, resulting in features that are not as abstract ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=either%20at%20the%20pixel%20or,full%20advantage%20of%20these%20methods)). Empirically, **I-JEPA’s representations have been shown to outperform those from MAE in downstream tasks requiring semantic understanding** (e.g. ImageNet classification) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,1K%2C%20and%20semantic%20transfer%20tasks)). They are on par with or better than contrastive methods on many tasks, and notably **retain strengths in tasks requiring spatial precision**, like object counting or depth prediction, where I-JEPA outshines invariance-based approaches ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,a%20wider%20set%20of%20tasks)) (likely because it does not explicitly make the model invariant to shifts or scale, thus preserving spatial cues).

- **Generalization Ability:** A key promise of self-supervised learning is to generalize to new tasks or data regimes with minimal labels. I-JEPA’s avoidance of hand-crafted augmentations gives it a versatility across domains – it does not bake in assumptions specific to ImageNet-like images (e.g. that color or orientation is irrelevant), which makes it easier to transfer to different data types. For instance, augmentations used in SimCLR (like heavy crops) may not translate to domains like driving, where preserving the whole scene is important ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=of%20a%20high%20semantic%20level%C2%A0,other%20modalities%20such%20as%20audio)). In contrast, I-JEPA’s strategy of predicting missing parts can be applied to images, audio, LiDAR, etc., without manual tuning of augmentations ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=generalize%20these%20biases%20for%20tasks,other%20modalities%20such%20as%20audio)). In fact, the *same I-JEPA idea was successfully extended to LiDAR data* for autonomous driving by simply predicting bird’s-eye-view embeddings of point clouds ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=Predictive%20Architecture%29%2C%20a%20novel%20self,We%20qualitatively)). This indicates strong generalization of the approach across modalities. Moreover, I-JEPA tends to learn more “complete” representations – not only high-level class information (as contrastive methods do for classification), but also some low-level or layout information (as evidenced by its performance on depth estimation) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,a%20wider%20set%20of%20tasks)). This broad capture of information can translate to better generalization on varied tasks. BYOL and SimCLR are very effective for recognition tasks, but can struggle if the downstream task requires sensitivity to features they made invariant (e.g., if a downstream task needs color or absolute scale, a contrastively trained model that ignored those during pre-training might underperform) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=amongst%20others%C2%A0,is%20not%20straightforward%20to%20generalize)). MAE, conversely, might generalize poorly to very high-level tasks without fine-tuning, since its features are not as ready-made semantic; it often needs heavier fine-tuning to adapt ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=modality%C2%A0,full%20advantage%20of%20these%20methods)). In summary, I-JEPA offers a good trade-off, yielding representations that transfer well with minimal tuning (e.g., linear probes or few-shot) across a range of tasks ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,1K%2C%20and%20semantic%20transfer%20tasks)) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,a%20wider%20set%20of%20tasks)). For example, with only 1% of ImageNet labels, a large I-JEPA model exceeded the accuracy of SimCLR and BYOL on ImageNet by several points ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=iBOT%C2%A0%5B79%20%5D%20ViT,pretraining%20outperforms%20MAE%20which%20also)) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=MSN%C2%A0%5B4%5D%20ViT,augmentations)), demonstrating label-efficient generalization.

- **Computational Efficiency:** I-JEPA is designed for efficiency in pre-training. Since it only processes one set of context patches from each image (plus a relatively small predictor network), it avoids redundant computation. **Contrastive methods like SimCLR/DINO double the workload by processing two augmented views per image**, and also require large batch sizes or memory banks to store negatives – this can be computationally heavy, hindering scalability ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=semantic%20image%20representations,Small%2F16%20trained%20with%20iBOT)). BYOL halves the batch size requirement (no negatives) but still uses two forward passes (online and target networks) and sophisticated mechanisms to prevent collapse. I-JEPA also uses two networks (context and target encoders), but the target encoder is typically a momentum copy not adding much overhead, and multiple target patches can be predicted from one context in parallel. The I-JEPA paper noted that **training a huge ViT-H/14 with I-JEPA was more efficient (in GPU-hours) than training even a small ViT-S/16 with a state-of-the-art contrastive method (iBOT) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=style%20patch,Small%2F16%20trained%20with%20iBOT))**. This efficiency comes partly from avoiding *view augmentations*: methods like DINO or SimCLR effectively see ~2× images and also often use extra projector heads, while I-JEPA’s predictor head is lightweight ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,supervised%20pretraining)). Generative methods like MAE incur cost in the decoder: MAE uses a large decoder to rebuild pixels, which adds to training time. I-JEPA has no large decoder – prediction is done by a small network operating on latent features, significantly cutting compute ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,supervised%20pretraining)). Additionally, not relying on contrastive negatives or large augmentation pipelines simplifies the training pipeline. Overall, I-JEPA achieves **better throughput and scalability**, enabling use of larger models or longer training on the same budget ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,supervised%20pretraining)). This is evidenced by the ability to train high-capacity models quickly and observe performance continue to improve with model size ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Methods%20using%20extra%20view%20data,invariance%20approaches)) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=pretrained%20at%20at%20a%20resolution,augmentations)).

- **Performance on Vision Tasks:** On standard vision benchmarks, I-JEPA has proven highly competitive. For image classification (ImageNet-1k linear probe), a large I-JEPA model (ViT-H/14) reached **81.1% top-1 accuracy**, matching the performance of the best contrastive methods (e.g. iBOT with 81.0%) despite using no augmentations during pre-training ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Methods%20using%20extra%20view%20data,invariance%20approaches)). Even smaller I-JEPA models outperform comparably sized MAE models by a significant margin in classification ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Method%20Arch.%20Epochs%20Top,B%2F16%20600%2072.9)) ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=Table%201%3A%20ImageNet.%20Linear,augmentations)). Beyond classification, I-JEPA’s representations excel in *low-shot and transfer settings*. For instance, with only 1% of labels, I-JEPA outperformed SimCLR v2 and BYOL by ~6–7% absolute on ImageNet ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=iBOT%C2%A0%5B79%20%5D%20ViT,pretraining%20outperforms%20MAE%20which%20also)), showcasing strong resilience when labeled data is scarce. On tasks like object counting and depth prediction (which require understanding of scene layout or numerosity), I-JEPA surpassed contrastive learners ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,a%20wider%20set%20of%20tasks)). Meanwhile, MAE needed full fine-tuning to catch up on semantic tasks, whereas I-JEPA’s features were immediately useful for linear probes ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,1K%2C%20and%20semantic%20transfer%20tasks)). In sum, I-JEPA achieves **state-of-the-art or close to SOTA performance across a wide range of vision tasks**, bridging the gap between contrastive and generative approaches. It combines the strengths of both: approaching the semantic accuracy of contrastive methods on recognition tasks, while often **exceeding them on tasks requiring more than just class invariances** ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=I,a%20wider%20set%20of%20tasks)). This balanced performance, along with efficiency, is why I-JEPA is seen as a promising new paradigm in self-supervised vision learning ([[2301.08243] Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://ar5iv.labs.arxiv.org/html/2301.08243#:~:text=tasks%20and%20achieves%20better%20performance,a%20wider%20set%20of%20tasks)).

# Prior Work on I-JEPA in Autonomous Driving

## Applications of I-JEPA to Driving Tasks  
The I-JEPA paradigm has begun to be applied in the context of autonomous driving, primarily to improve **perception and world modeling** without relying on labeled data. A notable example is **AD-L-JEPA** (“Autonomous Driving with LiDAR via JEPA”) by Zhu et al. (2025) ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=)) ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=,learns%20spatial%20world%20models%20with)). AD-L-JEPA extends the joint-embedding predictive approach to **LiDAR point clouds**. Instead of camera images, it deals with 3D data by predicting **Bird’s-Eye View (BEV) embeddings** of a LiDAR sweep from part of the scene ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=Predictive%20Architecture%29%2C%20a%20novel%20self,We%20qualitatively)). Like I-JEPA, it is *neither generative nor contrastive* – it does not reconstruct raw point clouds (as a generative autoencoder would) and does not require paired augmentations or contrastive samples ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=Predictive%20Architecture%29%2C%20a%20novel%20self,JEPA%20leads%20to)). **Masked regions of the LiDAR scan are not filled in explicitly; rather, their latent representation in a BEV feature map is predicted from the visible regions** ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=neither%20generative%20nor%20contrastive,We%20qualitatively)). This approach yielded a form of **self-supervised spatial world model** for driving scenes. The authors report that **AD-L-JEPA learns high-quality LiDAR representations that transfer well to downstream tasks like 3D object detection** ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=simpler%20implementation%20and%20enhanced%20learned,training%20in%20autonomous%20driving)). In fact, the learned embeddings were evaluated on standard benchmarks (e.g. fine-tuning for KITTI or Waymo 3D object detection) and showed improved accuracy and data-efficiency compared to prior self-supervised methods ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=JEPA,available%20at%20this%20https%20URL)). Notably, AD-L-JEPA outperformed two recent state-of-the-art self-supervised LiDAR frameworks – **Occupancy-MAE** and **ALSO** – on downstream detection tasks ([[2501.04969] AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969#:~:text=JEPA,available%20at%20this%20https%20URL)). Occupancy-MAE is a masked autoencoder for LiDAR that reconstructs 3D occupancy grids, and ALSO uses occupancy completion as a self-supervised task; both were leading approaches before. AD-L-JEPA’s success indicates that the JEPA concept (learning by predicting latent representations) is effective for complex 3D, multi-object scenes in driving. A recent survey on world models for autonomous driving highlights AD-L-JEPA as *the first JEPA-based method for self-supervised LiDAR learning*, emphasizing that it **eliminates generative/contrastive mechanisms and speeds up pre-training compared to Occupancy-MAE and ALSO** ([A Survey of World Models for Autonomous Driving](https://arxiv.org/html/2501.11260v2#:~:text=JEPA%C2%A0%5B137%5D%20introduces%20a%20self,and%20explicit%20data%20reconstruction%20while)) ([A Survey of World Models for Autonomous Driving](https://arxiv.org/html/2501.11260v2#:~:text=capturing%20occluded%20or%20uncertain%20environmental,learning%20even%20with%20partially%20randomized)). In summary, AD-L-JEPA demonstrates JEPA’s viability for learning **spatial representations of driving environments** that can reduce the need for labeled 3D data.

Beyond LiDAR, applications of I-JEPA to camera-based driving data are still emerging. As of now, there is limited published work explicitly applying I-JEPA to autonomous driving **camera imagery or end-to-end driving policies**. However, related research provides hints of I-JEPA’s potential. For example, some works have used *self-supervised vision models as pre-training for driving*. A 2024 study by Juneja et al. used **DINO (a BYOL/contrastive hybrid)** to pre-train a vision transformer for the CARLA driving simulator, and found it improved imitation learning performance compared to training from scratch or using ImageNet labels ([[2407.10803] DINO Pre-training for Vision-based End-to-end Autonomous Driving](https://arxiv.org/abs/2407.10803#:~:text=,is%20on%20par%20with%20the)). This suggests that *rich semantic pre-trained features* (like those I-JEPA would provide) can benefit driving tasks. In another vein, **ACT-JEPA** (Vujinovic et al., 2025) incorporated a JEPA-like predictor into imitation learning: it trained a policy to predict sequences of future observations in latent space alongside actions ([(PDF) ACT-JEPA: Joint-Embedding Predictive Architecture Improves Policy Representation Learning](https://www.researchgate.net/publication/388402411_ACT-JEPA_Joint-Embedding_Predictive_Architecture_Improves_Policy_Representation_Learning#:~:text=we%20propose%20ACT,model%27s%20ability%20to%20predict%20abstract)). While not specific to autonomous driving, it showed that **predictive representations can improve policy learning** by capturing environment dynamics ([(PDF) ACT-JEPA: Joint-Embedding Predictive Architecture Improves Policy Representation Learning](https://www.researchgate.net/publication/388402411_ACT-JEPA_Joint-Embedding_Predictive_Architecture_Improves_Policy_Representation_Learning#:~:text=representation%20space%2C%20allowing%20the%20model,making%20tasks)). These developments imply that I-JEPA could be naturally extended to driving videos or sequences, allowing an agent to predict what it will see next, thereby learning a stronger world model for planning.

## Gaps and Potential Improvements  
Despite these early successes, there are several **gaps and opportunities** in applying I-JEPA to autonomous driving tasks:

- **Camera-Based JEPA for Driving:** Currently, the direct application of I-JEPA on *camera sensor data* in driving (e.g. dashcam or front-view images) is under-explored. AD-L-JEPA tackled LiDAR; an analogous approach for images (predicting representations of masked regions of a driving scene image) could be developed. This could help learn visual features for road scenes (lanes, vehicles, pedestrians) without labels. A gap remains in evaluating how I-JEPA pre-trained on driving images compares to other image SSL methods for tasks like semantic segmentation or detection of traffic elements.

- **Temporal Predictive Modeling:** I-JEPA in its original form deals with static images. Autonomous driving, however, is inherently temporal – the system sees video streams. An important extension would be a **temporal JEPA**, where the context could be one or several video frames and the target could be future frames’ embeddings. This would let the model learn motion dynamics and predictive features (e.g. anticipating how a pedestrian might move). So far, most JEPA work (image or LiDAR) has focused on single snapshots ([A Survey of World Models for Autonomous Driving](https://arxiv.org/html/2501.11260v2#:~:text=JEPA%C2%A0%5B137%5D%20introduces%20a%20self,and%20explicit%20data%20reconstruction%20while)). Incorporating time (predicting forward in time in latent space) is a promising improvement to build predictive world models useful for planning. Such an extension could draw on ideas from predictive world-model RL (e.g. using latent dynamics as in *Think2Drive* ([Think2Drive: Efficient Reinforcement Learning by Thinking with Latent World Model for Autonomous Driving (in CARLA-v2)](https://arxiv.org/html/2402.16720v2#:~:text=so%20far%20no%20literature%20has,level%20proficiency%20in%20CARLA%20v2)) ([Think2Drive: Efficient Reinforcement Learning by Thinking with Latent World Model for Autonomous Driving (in CARLA-v2)](https://arxiv.org/html/2402.16720v2#:~:text=far%20there%20is%20no%20reported,infraction%20number%2C%20and%20scenario%20density))) but using the JEPA principle to train those dynamics self-supervised.

- **Multi-Modal Fusion:** Another gap is applying I-JEPA in a **multi-sensor context**. An autonomous vehicle uses cameras, LiDAR, radar, etc. Self-supervised learning across modalities could be powerful – e.g., using camera images as context and LiDAR as target (or vice versa) to predict one modality’s embedding from another, forcing a shared representation. While JEPA has been individually applied to images and LiDAR, a joint model could learn to fuse sensor information implicitly. This could improve robustness (say, predicting a LiDAR scene embedding from a camera view might help the model learn cross-modal features and handle sensor failures). Research has yet to explore such cross-modal JEPA training in depth.

- **Integration with Policy Learning:** So far, JEPA-based pre-training has mostly been evaluated on perception metrics (classification, detection). A gap exists in demonstrating its impact on **driving policy performance** (steering, navigation decisions). Bridging this gap might involve using an I-JEPA pretrained backbone in an imitation or reinforcement learning pipeline (as hinted by ACT-JEPA’s approach of predicting observation sequences ([(PDF) ACT-JEPA: Joint-Embedding Predictive Architecture Improves Policy Representation Learning](https://www.researchgate.net/publication/388402411_ACT-JEPA_Joint-Embedding_Predictive_Architecture_Improves_Policy_Representation_Learning#:~:text=extends%20this%20idea%20of%20chunking,making%20tasks))). The potential benefit is improved sample efficiency – a policy network initialized with a rich latent world model might need far fewer driving demonstrations to learn to drive. Exploring how to fine-tune or adapt I-JEPA’s features for end-to-end control (and whether additional objectives, such as predicting vehicle dynamics, should be added) is an open research direction.

In summary, prior work has shown I-JEPA’s promise for *self-supervised perception* in autonomous driving (especially with LiDAR ([A Survey of World Models for Autonomous Driving](https://arxiv.org/html/2501.11260v2#:~:text=JEPA%C2%A0%5B137%5D%20introduces%20a%20self,and%20explicit%20data%20reconstruction%20while))). The next steps will involve extending these ideas to camera data and temporal sequences, combining modalities, and tightly coupling the learned representations with the driving policy. These improvements aim to move from representations that feed into detection models toward representations that directly facilitate safe and robust driving decisions.

# Adapting I-JEPA for CARLA-Based Self-Driving

Adapting I-JEPA to a driving simulator like **CARLA** involves leveraging its pre-training strengths and then fine-tuning for the specific control task (steering). The goal is to obtain a high-performing driving policy with minimal manual labels by using self-supervised pre-training.

## Pre-Training on Diverse Driving Datasets  
The first step is to **pre-train I-JEPA on diverse driving data**. This could include a mix of real-world driving datasets (for realism) and simulated data (for scale and variety). For example, one might use large collections of unlabelled driving videos or images from the web, dashcam videos, or datasets like BDD100K, nuScenes, or Waymo Open Dataset. Diversity is key – covering various cities, weather, times, and traffic situations so that the model learns a broad “common-sense” understanding of driving environments. I-JEPA can be applied to this data by, say, taking each video frame (or a pair of frames) and masking out patches (e.g. hide parts of the road scene such as an area covering a traffic sign or a car) and training the network to predict the representation of those patches. By doing this at scale, the **context encoder** will learn features that encode vehicles, lanes, road semantics, etc., because it needs that knowledge to infer the missing pieces. Importantly, **no human-provided labels (like bounding boxes or segmentations) are needed during this pre-training** – the supervision comes from the image itself. This aligns with recent works that exploit unlabeled driving videos: for instance, the PPGeo framework showed that self-supervised geometric tasks on unlabeled YouTube driving videos improved learned policy features ([[2301.01006] Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling](https://arxiv.org/abs/2301.01006#:~:text=Geometric%20modeling,As%20such%2C%20the%20pre)) ([[2301.01006] Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling](https://arxiv.org/abs/2301.01006#:~:text=generates%20pose%20and%20depth%20predictions,with%20very%20limited%20data)). Similarly, an I-JEPA pre-trained on a large unlabeled corpus should encode useful driving-relevant features (e.g. recognizing road edges, vehicles, pedestrians and their likely context). The pre-training can also be done directly within CARLA by generating many scenarios in simulation (different maps, traffic patterns) and using I-JEPA on the rendered images – effectively **building a synthetic pre-training dataset** that is unlimited in size. After pre-training, we obtain a **backbone network (e.g. a ResNet or ViT) with weights that encapsulate driving-scene knowledge** in its embeddings.

## Fine-Tuning for Steering Control in CARLA  
Once we have a pre-trained I-JEPA model, we can fine-tune it for the specific task of **steering control in the CARLA simulator**. In CARLA, this typically means an **imitation learning setup**: we have some expert driving trajectories (e.g. recorded by a human or an expert policy) that provide inputs (camera images, maybe speed) and target outputs (steering angle, throttle, brake). Fine-tuning involves training a policy head on top of the frozen or partially fine-tuned I-JEPA encoder to map the encoded state to a driving action. Because the encoder is already rich in features, we can use a relatively small amount of labeled data for this. This is where **label-efficient fine-tuning techniques** come in. One approach is **freeze the bulk of the encoder** and only train a small number of final layers (or an adapter) on the labeled demonstrations (“linear probing” or shallow tuning). This reduces the risk of overfitting when labels are few. Another approach is **progressive tuning**, where one gradually unfreezes layers of the encoder as needed (so the model doesn’t forget its pre-trained features too quickly). Additionally, **data augmentation and synthetic data generation** in CARLA can supplement real demos – since CARLA can simulate new scenarios, one can generate extra training situations and have a basic autopilot or safety oracle provide noisy supervision, then refine using the pre-trained model (a form of self-training). The expectation is that an I-JEPA-pretrained model will significantly outperform a from-scratch model when both are given the same small fine-tuning set. Indeed, evidence from analogous studies supports this: using DINO-pretrained features in CARLA imitation learning yielded better driving performance than using a random or ImageNet-initialized model ([[2407.10803] DINO Pre-training for Vision-based End-to-end Autonomous Driving](https://arxiv.org/abs/2407.10803#:~:text=on%20a%20classification,VPRPre)). Furthermore, self-supervised pre-training can help the policy be **more robust** – e.g., the model has already seen various visual conditions, so it’s less likely to be fooled by a slight change in lighting or by a new object shape during deployment.

**Label-efficient fine-tuning** can be quantified in simulation by how much expert data is needed to achieve a certain driving performance. With I-JEPA, we expect to need far fewer episodes. For example, if training from scratch requires, say, 10 hours of driving data to reach an 80% success rate on a set of routes, a pre-trained model might achieve the same with only 2–3 hours of data. This kind of gain has precedent: in the perception domain, pre-training on unlabeled data has halved the requirement of labeled data for detection ([[2206.09900] Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders](https://arxiv.org/abs/2206.09900#:~:text=the%20effectiveness%20of%20Occupancy,For%203D%20semantic%20segmentation)). Specifically, the Occupancy-MAE LiDAR pre-training was shown to **reduce labeled 3D data needs by 50% for object detection** in KITTI ([[2206.09900] Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders](https://arxiv.org/abs/2206.09900#:~:text=the%20effectiveness%20of%20Occupancy,For%203D%20semantic%20segmentation)). We anticipate similar gains for control. In practice, one could conduct experiments in CARLA’s benchmark tasks (like the CARLA Leaderboard routes or the older NoCrash benchmark) comparing agents trained with and without I-JEPA pre-training. Key techniques to maximize label-efficiency include: **(a)** using *early stopping* to avoid overfitting the small fine-tuning set, **(b)** applying **domain randomization** during fine-tuning (since the model can handle variability, we can randomize weather, slight camera angles, etc., to augment the data), and **(c)** possibly leveraging **reinforcement learning fine-tuning** (for example, use the pre-trained model as the policy network and fine-tune it with an RL reward for route completion, which can further improve performance without direct supervision everywhere). The combination of these techniques, grounded by a strong I-JEPA prior, should yield a policy that drives well in CARLA with far fewer labeled examples.

In summary, adapting I-JEPA to CARLA involves *pre-training on diverse driving scenes (real or simulated) to learn general visual representations*, then *efficiently fine-tuning that knowledge for the specific task of controlling a vehicle*. By doing so, we leverage the richness of self-supervised features to achieve robust driving with minimal supervised data – a step toward more scalable autonomous driving research. This approach also naturally aligns with CARLA’s use as a testing ground: we can quantitatively evaluate how pre-training impacts driving metrics, as discussed next.

# Evaluation Metrics and Benchmarks

## Self-Supervised Learning Evaluation Metrics in Autonomous Driving  
Evaluating self-supervised learning for autonomous driving spans both **representation quality metrics** and **driving performance metrics**:

- **Downstream Task Performance:** A primary way to assess the quality of learned features is to fine-tune or evaluate them on relevant downstream tasks. For perception-oriented representations, this includes metrics like **classification accuracy**, **object detection Average Precision (AP)**, and **segmentation mean IoU** on standard datasets. For example, a self-supervised model might be evaluated by training a 3D object detector on KITTI or nuScenes – metrics would be mAP for cars, pedestrians, etc. If the model was pre-trained well, it should achieve higher mAP or require fewer epochs/labels to reach a baseline. In the case of AD-L-JEPA, they likely report 3D detection AP on datasets like KITTI or Waymo; improvement over baseline AP would validate the approach ([[2206.09900] Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders](https://arxiv.org/abs/2206.09900#:~:text=the%20effectiveness%20of%20Occupancy,For%203D%20semantic%20segmentation)). Another downstream metric is **distance error in depth prediction** (if evaluating representations on monocular depth estimation tasks), or **IoU in segmentation of roads/vehicles**. The key is that higher downstream metrics, or matching performance with much less labeled data, indicates a better self-supervised representation.

- **Label Efficiency:** As mentioned, one metric is the reduction in labeled data needed to achieve a given performance. This can be reported as a percentage. For instance, *Occupancy-MAE’s result of cutting required labels by 50% for car detection* ([[2206.09900] Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders](https://arxiv.org/abs/2206.09900#:~:text=the%20effectiveness%20of%20Occupancy,For%203D%20semantic%20segmentation)) is a concrete metric. Similarly, we might measure how a pre-trained model performs when fine-tuned on N% of the data versus a baseline. The accuracy at low-label regimes (like 1% or 10% of data) is a common metric (used in ImageNet linear evaluation, and also in driving – e.g., how well does a policy perform with only 10 minutes of expert data).

- **Prediction Accuracy (Pretext Task):** During training of a JEPA model, one can monitor the loss or accuracy of predicting the target embeddings. While this isn’t an end metric, it can be useful to measure *how well the model predicts withheld content*. A lower prediction error on masked regions suggests the model is capturing the scene structure. Some works might also evaluate **proxy tasks**: for example, AD-L-JEPA could evaluate how well the latent prediction captures occluded objects by checking if the predicted BEV embedding can be used to reconstruct or classify occluded vs visible regions ([A Survey of World Models for Autonomous Driving](https://arxiv.org/html/2501.11260v2#:~:text=JEPA%C2%A0%5B137%5D%20introduces%20a%20self,and%20explicit%20data%20reconstruction%20while)). However, these internal metrics are secondary to downstream evaluation.

- **Robustness Tests:** Autonomous driving requires robustness to varying conditions. For representations, robustness can be evaluated by testing the model’s transfer to different domains or its performance under perturbations. For example, one might pre-train on clear weather images and test representation quality on nighttime or rainy images – measuring drop in detection accuracy or segmentation quality. Another robustness metric is the model’s performance under **synthetic distortions or adversarial noise** (how much does accuracy drop if the input is slightly corrupted). For driving policies, robustness is measured by generalization to new towns or traffic situations in CARLA that were not seen in training. A self-supervised pre-trained policy might maintain higher driving success in an unseen town than a specialized policy, indicating robust, generalizable features.

- **Safety Metrics:** In the context of driving control, we evaluate safety-critical metrics. These include **number of collisions**, **infractions**, or **interventions** over test episodes. In CARLA specifically, standard safety metrics are defined in the leaderboard: infractions like collisions with pedestrians, vehicles, running red lights, etc., each incur a penalty ([ALPHA DRIVE |   CARLA Leaderboard Case Study](https://alphadrive.ai/industries/automotive/carla-leaderboard-case-study/#:~:text=Infractions)). A safe driving agent should minimize these. We often combine them into a single score; for instance, the CARLA leaderboard defines an **“Infraction Penalty”** which multiplies penalties for each infraction type ([ALPHA DRIVE |   CARLA Leaderboard Case Study](https://alphadrive.ai/industries/automotive/carla-leaderboard-case-study/#:~:text=distance%20completed%20by%20an%20agent%2C,for%20every%20instance%20of%20these)). A self-supervised approach that leads to a better policy would yield fewer infractions per kilometer. Additionally, one can look at **off-road occurrences** or whether the agent respects traffic lanes – metrics like “percentage of route driven off-road” or “centerline deviation” can quantify safety. In simulation, “disengagements” (where an auto driving fails and a human or script takes over) can be counted, though in CARLA evaluations this is analogous to route failure cases.

- **Driving Task Success:** For end-to-end policies, metrics like **Route Completion** and **Driving Score** are crucial. **Route Completion** is the percentage of the route (from start to goal) the agent successfully travels without deviating ([ALPHA DRIVE |   CARLA Leaderboard Case Study](https://alphadrive.ai/industries/automotive/carla-leaderboard-case-study/#:~:text=the%20infraction%20penalty%20of%20the,for%20every%20instance%20of%20these)). **Driving Score** is an aggregate used in CARLA’s benchmark: it’s essentially route completion penalized by infractions ([ALPHA DRIVE |   CARLA Leaderboard Case Study](https://alphadrive.ai/industries/automotive/carla-leaderboard-case-study/#:~:text=,an%20agent%20as%20a%20geometric)). A driving score of 1.0 means the agent completed all routes with no infractions; lower scores indicate issues. Self-supervised learned agents are evaluated on these – e.g., “our agent achieved X% route completion and a driving score of Y, compared to baseline’s Z”. These metrics directly reflect an agent’s proficiency in autonomous driving tasks and are thus the ultimate test of whether self-supervised learning helped the driving system be more effective and safe.

In summary, we measure self-supervised learning success in autonomous driving by how well the learned representations improve **downstream task metrics** (accuracy, AP, IoU, etc.), how **data-efficient** they make the training, and how they impact the **robustness and safety** of driving performance (collisions, infractions, route success). A combination of these gives a holistic view: for instance, an ideal approach yields high detection accuracy with few labels, and when used in a driving policy, results in high driving scores and low infraction rates in diverse scenarios.

## Key Datasets and Benchmarking Approaches (Relevant to CARLA)  
A variety of datasets and benchmarks are used to evaluate self-supervised learning for autonomous driving, each focusing on different aspects:

- **Real-World Driving Datasets:** These are used both for pre-training and for evaluating transfer learning. **KITTI** is a common benchmark for 3D object detection and stereo/optical flow; self-supervised methods often evaluate how pre-training helps 3D detection on KITTI (e.g., higher mAP or ability to use fewer training examples) ([[2206.09900] Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders](https://arxiv.org/abs/2206.09900#:~:text=the%20effectiveness%20of%20Occupancy,For%203D%20semantic%20segmentation)). **nuScenes** and **Waymo Open Dataset** are larger-scale sets with camera + LiDAR; they provide benchmarks for 3D detection, tracking, and segmentation. For instance, Occupancy-MAE and AD-L-JEPA evaluate on nuScenes or Waymo to show improvement in detecting cars and pedestrians ([[2206.09900] Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders](https://arxiv.org/abs/2206.09900#:~:text=the%20effectiveness%20of%20Occupancy,available%20at%20this%20https%20URL)). **Cityscapes** or **BDD100K** can be used to assess semantic segmentation performance – a self-supervised model can be fine-tuned on these to measure mIoU gains. These real datasets ensure that the learned representations genuinely help in realistic scenarios.

- **CARLA Simulation Benchmarks:** CARLA itself is a platform for controlled experiments. One famous benchmark is the **CARLA Autonomous Driving Leaderboard**, which defines a set of routes through a town with traffic and pedestrians. Agents are evaluated on **Driving Score, Route Completion, and Infractions** as described above ([ALPHA DRIVE |   CARLA Leaderboard Case Study](https://alphadrive.ai/industries/automotive/carla-leaderboard-case-study/#:~:text=,an%20agent%20as%20a%20geometric)). The Leaderboard provides a public ranking, and it has been used in research to test new driving policies. For example, in the DINO pre-training study, the authors likely used the CARLA Leaderboard or a similar set of routes to compare performance of agents with and without pre-training ([[2407.10803] DINO Pre-training for Vision-based End-to-end Autonomous Driving](https://arxiv.org/abs/2407.10803#:~:text=on%20a%20classification,VPRPre)). The Leaderboard is quite challenging, including new weather conditions and unforeseen events, so it is an excellent test of generalization. Another benchmark is the **NoCrash benchmark**, introduced in earlier CARLA research, which evaluates driving in empty, regular, and dense traffic settings across training and new towns. The success rate (percentage of episodes completed without crashing) is the metric there. This was used in many imitation learning papers; a self-supervised enhanced policy should achieve higher success especially in the “dense traffic” condition (which tests generalization under heavy traffic). CARLA also allows custom scenario evaluations – e.g., **Corner Case scenarios** (as in the *Think2Drive* work, which introduced a CornerCaseRepo to test specific rare events ([Think2Drive: Efficient Reinforcement Learning by Thinking with Latent World Model for Autonomous Driving (in CARLA-v2)](https://arxiv.org/html/2402.16720v2#:~:text=involves%2039%20new%20common%20events,RL%20thanks%20to%20the%20low)) ([Think2Drive: Efficient Reinforcement Learning by Thinking with Latent World Model for Autonomous Driving (in CARLA-v2)](https://arxiv.org/html/2402.16720v2#:~:text=model,infraction%20number%2C%20and%20scenario%20density))). These scenarios and corresponding success metrics can probe the model’s robustness in unusual conditions (like sudden pedestrian crossings, etc.).

- **Synthetic Data for Pre-training:** CARLA can generate synthetic datasets with ground-truth annotations at scale. For instance, one can create a dataset of 100k images with ground truth segmentation. Although this is labeled data, it can serve as a benchmark to evaluate representation learning: e.g., train a segmentation model on a small fraction of this synthetic data using the pre-trained encoder vs. scratch, and measure mIoU on a test set. This isolates the effect of representation quality in a controlled way. Additionally, one can simulate sensor degradation or domain shift in CARLA (changing camera calibration or adding sensor noise) to test how well features cope – a form of benchmark for domain robustness.

- **Metrics and Datasets Tied Together:** It’s worth noting that benchmarks often define their own primary metrics. For example, nuScenes defines a mAP and a NuScenes detection score for 3D detection. The CARLA Leaderboard defines the driving score. When reporting results, researchers typically report the standard metric from that benchmark. So a literature review will note things like “Method X achieves Y% mAP on nuScenes detection” or “completes Z% of routes in CARLA Town05 with no collisions” to quantify performance.

In context of CARLA and self-supervised learning, a typical experimental pipeline might be: pre-train on a large dataset (either real or simulated), fine-tune on a small labeled set (either real data for perception tasks or a few expert demonstrations for policy), then evaluate on **CARLA’s driving benchmark** for control or on **KITTI/nuScenes** for perception. Success would be demonstrated by surpassing baseline scores on those benchmarks. For instance, AD-L-JEPA was shown to outperform **Occupancy-MAE** and **ALSO** on 3D detection benchmarks ([A Survey of World Models for Autonomous Driving](https://arxiv.org/html/2501.11260v2#:~:text=capturing%20occluded%20or%20uncertain%20environmental,learning%20even%20with%20partially%20randomized)), and a hypothetical I-JEPA-for-camera model could be tested on CARLA’s driving routes to see if it achieves a higher driving score than prior imitation learners.

In summary, the evaluation of self-supervised methods for autonomous driving is comprehensive: we use real datasets like KITTI, nuScenes, Waymo to evaluate improvements in detection/segmentation (with metrics like AP, IoU), and we use CARLA simulation benchmarks to evaluate end-to-end driving performance (metrics like Driving Score, route completion, collisions). Together, these benchmarks ensure that a method like I-JEPA not only *learns good representations* in theory but also *translates to safer and more effective driving* in practice ([ALPHA DRIVE |   CARLA Leaderboard Case Study](https://alphadrive.ai/industries/automotive/carla-leaderboard-case-study/#:~:text=,an%20agent%20as%20a%20geometric)) ([A Survey of World Models for Autonomous Driving](https://arxiv.org/html/2501.11260v2#:~:text=capturing%20occluded%20or%20uncertain%20environmental,learning%20even%20with%20partially%20randomized)). Each positive result on these benchmarks strengthens the case for self-supervised learning as a cornerstone of autonomous vehicle intelligence.
